{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e325dc98",
   "metadata": {},
   "source": [
    "# Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE, RFECV, mutual_info_regression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc382aa",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24068664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_preprocessing:\n",
    "    def __init__(self, train_filepath, test_filepath):\n",
    "        self.train_filepath = train_filepath\n",
    "        self.test_filepath = test_filepath\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        train_df = pd.read_csv(self.train_filepath)\n",
    "        test_df = pd.read_csv(self.test_filepath)\n",
    "        return train_df, test_df\n",
    "    \n",
    "    def extract_labels(self):\n",
    "        train_df, test_df = self.load_dataset()\n",
    "        columns = list(train_df.columns)\n",
    "        return columns[len(columns)-3:]\n",
    "    \n",
    "    def categorical_to_numeric(self):\n",
    "        train_df, test_df = self.load_dataset()\n",
    "        categorical_features = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "        binary_features = []\n",
    "        \n",
    "        for i in categorical_features:\n",
    "            if len(train_df[i].unique()) == 2:\n",
    "                categories = train_df[i].unique().tolist()\n",
    "                train_df[i].replace({categories[0]: 0, categories[1]: 1}, inplace=True)\n",
    "                test_df[i].replace({categories[0]: 0, categories[1]: 1}, inplace=True)\n",
    "                \n",
    "        # one hot encoding other nominal data\n",
    "        nominal_features = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "        tr_df = train_df.drop(nominal_features, axis=1)\n",
    "        ts_df = test_df.drop(nominal_features, axis=1)\n",
    "        encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "        encoder.fit(train_df[nominal_features])\n",
    "        encoder.categories_\n",
    "        encoder.transform(train_df[nominal_features])\n",
    "        train_df = pd.concat([train_df, pd.DataFrame(encoder.transform(train_df[nominal_features]))], axis=1)\n",
    "        \n",
    "        encoder.fit(test_df[nominal_features])\n",
    "        encoder.categories_\n",
    "        encoder.transform(test_df[nominal_features])\n",
    "        test_df = pd.concat([test_df, pd.DataFrame(encoder.transform(test_df[nominal_features]))], axis=1)\n",
    "        \n",
    "        return train_df, test_df, nominal_features, tr_df, ts_df\n",
    "    \n",
    "    def train_test_sets(self, prior):\n",
    "        train_df, test_df, nominal_features, tr_df, ts_df = self.categorical_to_numeric()\n",
    "        train_df.drop(nominal_features, axis=1, inplace=True)\n",
    "        test_df.drop(nominal_features, axis=1, inplace=True)\n",
    "        \n",
    "        train_labels = np.array(train_df[grades])\n",
    "        train_df = train_df.drop(grades, axis=1)\n",
    "        \n",
    "        test_labels = np.array(test_df[grades])\n",
    "        test_df = test_df.drop(grades, axis=1)\n",
    "        \n",
    "        if prior==True:\n",
    "#             train_set = np.array(train_df.drop(grades[2], axis=1))\n",
    "            train_labels = np.array(train_df[grades[2]])\n",
    "#             test_set = np.array(test_df.drop(grades[2], axis=1))\n",
    "            test_labels = np.array(test_df[grades[2]])\n",
    "        \n",
    "        return train_labels, test_labels, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba801a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"student_performance_train.csv\"\n",
    "test_path = \"student_performance_test.csv\"\n",
    "\n",
    "preprocessor = Data_preprocessing(train_path, test_path)\n",
    "og_train_df, og_test_df = preprocessor.load_dataset()\n",
    "grades = preprocessor.extract_labels()\n",
    "train_data_df, test_data_df, nom_feats, tr_df, ts_df = preprocessor.categorical_to_numeric()\n",
    "labels, test_labels, train_df, test_df = preprocessor.train_test_sets(prior=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57885600",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = ts_df.drop(grades, axis=1)\n",
    "tr_df = tr_df.drop(grades, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3e669",
   "metadata": {},
   "source": [
    "\n",
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c797444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the first period academic performance\n",
    "class Trivial_Regressor:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def predict(self, x_test):\n",
    "        pred = np.zeros((x_test).shape[0])\n",
    "        for i in range((x_test).shape[0]):\n",
    "            pred[i] = np.mean(self.y)\n",
    "        return pred\n",
    "    \n",
    "class OneNN:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def predict(self, test_point):\n",
    "        euclidean_dist = np.zeros((self.x).shape[0])\n",
    "        for i in range(len(self.x)):\n",
    "#             print(\"sum\", np.sqrt(np.sum((self.x[i, :] - test_point)**2)))\n",
    "            euclidean_dist[i] = np.sqrt(np.sum((self.x.iloc[i, :] - test_point)**2))\n",
    "#         print(euclidean_dist.shape)\n",
    "        neighbors = np.argsort(euclidean_dist) # calculating the closest neighbors\n",
    "        return np.mean(self.y[neighbors[:1]])\n",
    "\n",
    "# Performance Measures\n",
    "class error_estimation:\n",
    "    def __init__(self, y_true):\n",
    "        self.y_true = y_true\n",
    "\n",
    "    ## Root Mean square Error\n",
    "    def RMSE(self, y_pred):\n",
    "        return mean_squared_error(self.y_true, y_pred, squared=False)\n",
    "\n",
    "    ## Mean absolute error\n",
    "    def MAE(self, y_pred):\n",
    "        return mean_absolute_error(self.y_true, y_pred)\n",
    "\n",
    "    ## R-squared value\n",
    "    def r2(self, y_pred):\n",
    "        return r2_score(self.y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd472bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_dist(y_true, y_pred, m):   \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9,5))\n",
    "    ax1.hist(y_true, edgecolor='white')\n",
    "    ax1.set_title(f'Actual G1 Distribution')\n",
    "    ax2.hist(y_pred, edgecolor='white')\n",
    "    ax2.set_title(f'Predicted G1 Distribution')\n",
    "#     fig.suptitle(f'MISSION{m}')\n",
    "#     plt.savefig(f'Images/Results/M{m}_dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e2075",
   "metadata": {},
   "source": [
    "## MISSION 1 \n",
    "Predict first-period academic performance without any prior academic performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TRIVIAL REGRESSION ######\n",
    "\n",
    "regressor1 = Trivial_Regressor(tr_df, labels[:,0])\n",
    "y_pred1 = regressor1.predict(ts_df)\n",
    "error = error_estimation(test_labels[:, 0])\n",
    "print(f'******************* For Mission 1 *******************')\n",
    "print(f'--------- For Trivial Regressor ---------')\n",
    "print(f'RMSE: {error.RMSE(y_pred1)}')\n",
    "print(f'MAE: {error.MAE(y_pred1)}')\n",
    "print(f'R-squared: {error.r2(y_pred1)}')\n",
    "\n",
    "##### 1NN REGRESSION ######\n",
    "\n",
    "regressor2 = OneNN(tr_df, labels[:, 0])\n",
    "y_pred2 = np.zeros(test_labels.shape[0])\n",
    "for i in range(ts_df.shape[0]):\n",
    "    y_pred2[i] = regressor2.predict(np.array(ts_df)[i, :])\n",
    "print(f'--------- For 1NN Regressor ---------')\n",
    "print(f'RMSE: {error.RMSE(y_pred2)}')\n",
    "print(f'MAE: {error.MAE(y_pred2)}')\n",
    "print(f'R-squared: {error.r2(y_pred2)}')\n",
    "\n",
    "##### Linear Regression ######\n",
    "\n",
    "regressor3 = LinearRegression().fit(tr_df, labels[:, 0])\n",
    "y_pred3 = regressor3.predict(ts_df)\n",
    "print(f'--------- For Linear Regressor ---------')\n",
    "print(f'RMSE: {error.RMSE(y_pred3)}')\n",
    "print(f'MAE: {error.MAE(y_pred3)}')\n",
    "print(f'R-squared: {error.r2(y_pred3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ca9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_dist(labels[:,0], y_pred1, 1)\n",
    "plt.savefig(f'Images/Results/M1_trivial_dist.png')\n",
    "\n",
    "plot_pred_dist(labels[:,2], y_pred2, 2)\n",
    "plt.savefig(f'Images/Results/M1_1nn_dist.png')\n",
    "\n",
    "plot_pred_dist(labels[:,2], y_pred3, 3)\n",
    "plt.savefig(f'Images/Results/M1_linear_dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83410a9b",
   "metadata": {},
   "source": [
    "## MISSION 2\n",
    "Predict final-period academic performance without any prior academic performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ca7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TRIVIAL REGRESSION ######\n",
    "\n",
    "regressor1 = Trivial_Regressor(tr_df, labels[:,0])\n",
    "y_pred1 = regressor1.predict(ts_df)\n",
    "error = error_estimation(test_labels[:, 2])\n",
    "print(f'******************* For Mission 2 *******************')\n",
    "print(f'--------- For Trivial Regressor ---------')\n",
    "print(f'RMSE: {error.RMSE(y_pred1)}')\n",
    "print(f'MAE: {error.MAE(y_pred1)}')\n",
    "print(f'R-squared: {error.r2(y_pred1)}')\n",
    "\n",
    "##### 1NN REGRESSION ######\n",
    "\n",
    "regressor2 = OneNN(tr_df, labels[:, 2])\n",
    "# regressor2 = KNeighborsRegressor(n_neighbors=1).fit(tr_df, labels[:,2])\n",
    "# y_pred2 = regressor2.predict(ts_df)\n",
    "y_pred2 = np.zeros(test_labels.shape[0])\n",
    "for i in range(ts_df.shape[0]):\n",
    "    y_pred2[i] = regressor2.predict(np.array(ts_df)[i, :])\n",
    "    \n",
    "print(f'--------- For 1NN Regressor ---------')\n",
    "print(f'RMSE: {error.RMSE(y_pred2)}')\n",
    "print(f'MAE: {error.MAE(y_pred2)}')\n",
    "print(f'R-squared: {error.r2(y_pred2)}')\n",
    "\n",
    "##### Linear Regression ######\n",
    "\n",
    "regressor3 = LinearRegression().fit(tr_df, labels[:, 2])\n",
    "y_pred3 = regressor3.predict(ts_df)\n",
    "print(f'--------- For Linear Regressor ---------')\n",
    "print(f'RMSE: {error.RMSE(y_pred3)}')\n",
    "print(f'MAE: {error.MAE(y_pred3)}')\n",
    "print(f'R-squared: {error.r2(y_pred3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_dist(labels[:,0], y_pred1, 1)\n",
    "plt.savefig(f'Images/Results/M2_trivial_dist.png')\n",
    "\n",
    "plot_pred_dist(labels[:,2], y_pred2, 2)\n",
    "plt.savefig(f'Images/Results/M2_1nn_dist.png')\n",
    "\n",
    "plot_pred_dist(labels[:,2], y_pred3, 3)\n",
    "plt.savefig(f'Images/Results/M2_linear_dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a53cd0",
   "metadata": {},
   "source": [
    "## MISSION 3\n",
    "Predict final academic performance using all available prior academic performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3219d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df['G1'] = labels[:,0]\n",
    "tr_df['G2'] = labels[:,1]\n",
    "\n",
    "ts_df['G1'] = test_labels[:,0]\n",
    "ts_df['G2'] = test_labels[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e592851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### TRIVIAL REGRESSION ######\n",
    "\n",
    "regressor1 = Trivial_Regressor(tr_df, labels[:,2])\n",
    "y_pred1 = regressor1.predict(ts_df)\n",
    "error = error_estimation(test_labels[:,2])\n",
    "print(f'******************* For Mission 3 *******************')\n",
    "print(f'--------- For Trivial Regressor ---------')\n",
    "print(f'RMSE: {error.RMSE(y_pred1)}')\n",
    "print(f'MAE: {error.MAE(y_pred1)}')\n",
    "print(f'R-squared: {error.r2(y_pred1)}')\n",
    "\n",
    "##### 1NN REGRESSION ######\n",
    "\n",
    "regressor2 = OneNN(tr_df, labels[:,2])\n",
    "y_pred2 = np.zeros(test_labels.shape[0])\n",
    "for i in range(ts_df.shape[0]):\n",
    "    y_pred2[i] = regressor2.predict(np.array(ts_df)[i, :])\n",
    "print(f'--------- For 1NN Regressor ---------')\n",
    "print(f'RMSE: {error.RMSE(y_pred2)}')\n",
    "print(f'MAE: {error.MAE(y_pred2)}')\n",
    "print(f'R-squared: {error.r2(y_pred2)}')\n",
    "\n",
    "##### Linear Regression ######\n",
    "\n",
    "regressor3 = LinearRegression().fit(tr_df, labels[:,2])\n",
    "y_pred3 = regressor3.predict(ts_df)\n",
    "print(f'--------- For Linear Regressor ---------')\n",
    "print(f'RMSE: {error.RMSE(y_pred3)}')\n",
    "print(f'MAE: {error.MAE(y_pred3)}')\n",
    "print(f'R-squared: {error.r2(y_pred3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5857c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_dist(labels[:,0], y_pred1, 1)\n",
    "plt.savefig(f'Images/Results/M3_trivial_dist.png')\n",
    "\n",
    "plot_pred_dist(labels[:,2], y_pred2, 2)\n",
    "plt.savefig(f'Images/Results/M3_1nn_dist.png')\n",
    "\n",
    "plot_pred_dist(labels[:,2], y_pred3, 3)\n",
    "plt.savefig(f'Images/Results/M3_linear_dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce8c6d",
   "metadata": {},
   "source": [
    "# DATA VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32c80b8",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd1c35",
   "metadata": {},
   "source": [
    "### Histograms, Barplots, pieplots, KDE plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f619ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Final Grades - G1, G2, G3\n",
    "def grades_distribution(df, i):\n",
    "    plt.figure()\n",
    "    cmap = plt.get_cmap(\"tab20c\")\n",
    "    plt.hist(df[i], color=cmap(1), edgecolor='black')\n",
    "    plt.xlabel('Grade')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Final Grades')\n",
    "    plt.savefig(f'Images/Feature Selection/{i}_histogram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_distribution(og_train_df, 'G1')\n",
    "grades_distribution(og_train_df, 'G2')\n",
    "grades_distribution(og_train_df, 'G3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd48d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(15,5), sharey=True)\n",
    "for i in range(1, 4):\n",
    "    sns.kdeplot(data=og_train_df[grades].iloc[:,i-1], thresh=0.4, ax=axes[i-1])\n",
    "plt.suptitle(f'KDE plots of all the Grades')\n",
    "plt.savefig(f'Images/Feature Selection/KDE_grades.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a975ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_feat = og_train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "feat_df = og_train_df.drop(grades, axis=1)\n",
    "fig, ax = plt.subplots(6, 5, figsize=(15,11))\n",
    "fig.tight_layout()\n",
    "cmap = plt.get_cmap(\"tab20c\")\n",
    "colors = cmap(np.array([1, 5, 6, 9, 11]))\n",
    "for i, feat in enumerate(feat_df.columns):\n",
    "    if feat in categ_feat:\n",
    "        plt.sca(fig.axes[i])\n",
    "        feat_df[feat].value_counts().plot.pie(colors=colors)\n",
    "        fig.axes[i].axis('off')\n",
    "    else:\n",
    "        fig.axes[i].hist(feat_df[feat], color=cmap(1), edgecolor='black')\n",
    "    fig.axes[i].set_title(feat)\n",
    "plt.savefig(f'Images/Feature Selection/features_info.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c46e4e",
   "metadata": {},
   "source": [
    "### Readjusting the dataframes for better handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cadc43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(train_df, test_df):\n",
    "    grades1 = train_df['G1']\n",
    "    grades2 = train_df['G2']\n",
    "    grades3 = train_df['G3']\n",
    "    train_df = train_df.drop(grades, axis=1)\n",
    "    train_df.loc[:,'G1'] = grades1\n",
    "    train_df.loc[:,'G2'] = grades2\n",
    "    train_df.loc[:,'G3'] = grades3\n",
    "    \n",
    "    grades1 = test_df['G1']\n",
    "    grades2 = test_df['G2']\n",
    "    grades3 = test_df['G3']\n",
    "    test_df = test_df.drop(grades, axis=1)\n",
    "    test_df.loc[:,'G1'] = grades1\n",
    "    test_df.loc[:,'G2'] = grades2\n",
    "    test_df.loc[:,'G3'] = grades3\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df, new_test_df = train_test(train_data_df, test_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6496f3",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "### 1. Pearson Correlation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation Matrix\n",
    "def correlation_matrix(df, m):\n",
    "    fig, axes = plt.subplots(figsize = (20, 17))\n",
    "    feature_corr = df.corr()\n",
    "    sns.heatmap(feature_corr, mask=np.zeros_like(feature_corr),\n",
    "                cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "                square=True, ax=axes)\n",
    "    plt.title(f'Confusion Matrix for Mission{m+1}')\n",
    "    plt.savefig(f'Images/Feature Selection/correlation_matrix_{m}.png')\n",
    "    return feature_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b91b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove one of the two features whose correlation coefficient value is greater then 0.85\n",
    "def get_correlated_columns(df, c):\n",
    "    cols_corr = set() # set of all columns that are correlated\n",
    "    corr_matrix = df.corr()\n",
    "    for row in range(len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[row,-1]) < c:\n",
    "            cols_name = corr_matrix.columns[row] # get the column name\n",
    "            cols_corr.add(cols_name)\n",
    "    return cols_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = new_train_df.drop(nom_feats, axis=1)\n",
    "new_test_df = new_test_df.drop(nom_feats, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5fc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1 = new_train_df.drop(['G2', 'G3'], axis=1)\n",
    "test_df1 = new_test_df.drop(['G2', 'G3'], axis=1)\n",
    "\n",
    "train_df2 = new_train_df.drop(['G1', 'G2'], axis=1)\n",
    "test_df2 = new_test_df.drop(['G1', 'G2'], axis=1)\n",
    "\n",
    "train_df3 = new_train_df\n",
    "test_df3 = new_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mission1 - remove G2, G3 and predict G1\n",
    "X1 = train_df\n",
    "testX1 = test_df\n",
    "y1 = new_train_df[\"G1\"]\n",
    "testy1 = new_test_df[\"G1\"]\n",
    "features1 = X1.columns\n",
    "\n",
    "# Mission2 - remove G1, G2 and predict G3\n",
    "X2 = X1\n",
    "testX2 = test_df\n",
    "y2 = new_train_df[\"G3\"]\n",
    "testy2 = new_test_df[\"G3\"]\n",
    "features2 = X2.columns\n",
    "\n",
    "# Mission3 - set G1, G2 as features and predict G3\n",
    "X3 = new_train_df.drop(['G3'], axis=1)\n",
    "testX3 = new_test_df.drop(['G3'], axis=1)\n",
    "y3 = new_train_df[\"G3\"]\n",
    "testy3 = new_test_df[\"G3\"]\n",
    "features3 = X3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f995371",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_pearson = [train_df1, train_df2, train_df3]\n",
    "testX_pearson = [test_df1, test_df2, test_df3]\n",
    "threshold = 0.1\n",
    "sf_p = []\n",
    "for i in range(len(X_pearson)): \n",
    "    correlation_matrix(X_pearson[i], i)\n",
    "    correlated_features = get_correlated_columns(X_pearson[i], threshold)\n",
    "    print(f'Number of correlated features: {len(set(correlated_features))}')\n",
    "    print(f'correlated features for X{i+1}: {correlated_features}')\n",
    "    X_pearson[i] = X_pearson[i].drop(correlated_features, axis=1)\n",
    "    testX_pearson[i] = testX_pearson[i].drop(correlated_features, axis=1)\n",
    "    sf_p.append(X_pearson[i].columns)\n",
    "    \n",
    "X1_fs = X_pearson[0]\n",
    "X2_fs = X_pearson[1]\n",
    "X3_fs = X_pearson[2]\n",
    "\n",
    "testX1_fs = testX_pearson[0]\n",
    "testX2_fs = testX_pearson[1]\n",
    "testX3_fs = testX_pearson[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48793a8e",
   "metadata": {},
   "source": [
    "### 2. Univariate Feature Selection -Fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bcf718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_fregr(x1, x2, x3, t1, t2, t3, Y1, Y2, Y3, k):\n",
    "    sf_f = []\n",
    "    f_selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    f_selector1 = f_selector.fit(np.array(x1), Y1)\n",
    "    x1_f = f_selector1.transform(x1)\n",
    "    t1_f = f_selector1.transform(t1)\n",
    "    \n",
    "    selected_features1 = [features1[i] for i in f_selector1.scores_.argsort()[::-1][:k]]\n",
    "    sf_f.append(selected_features1)\n",
    "    \n",
    "    # Plot the scores\n",
    "    plt.figure()\n",
    "    plt.bar([i for i in range(len(f_selector1.scores_))], f_selector1.scores_)\n",
    "    plt.xlabel(f'feature index')\n",
    "    plt.ylabel(f'F-value')\n",
    "    plt.title(f'F-score Plots')\n",
    "    plt.savefig(f'Images/Feature Selection/F-scoreplots_1')\n",
    "    \n",
    "    f_selector2 = f_selector.fit(np.array(x2), Y2)\n",
    "    x2_f = f_selector2.transform(x2)\n",
    "    t2_f = f_selector2.transform(t2)\n",
    "    \n",
    "    selected_features2 = [features2[i] for i in f_selector2.scores_.argsort()[::-1][:k]]\n",
    "    sf_f.append(selected_features2)\n",
    "    \n",
    "    # Plot the scores\n",
    "    plt.figure()\n",
    "    plt.bar([i for i in range(len(f_selector2.scores_))], f_selector2.scores_)\n",
    "    plt.xlabel(f'feature index')\n",
    "    plt.ylabel(f'F-value')\n",
    "    plt.title(f'F-score Plots')\n",
    "    plt.savefig(f'Images/Feature Selection/F-scoreplots_2')\n",
    "    \n",
    "    f_selector3 = f_selector.fit(np.array(x3), Y3)\n",
    "    x3_f = f_selector3.transform(x3)\n",
    "    t3_f = f_selector3.transform(t3)\n",
    "    \n",
    "    selected_features3 = [features3[i] for i in f_selector3.scores_.argsort()[::-1][:k]]\n",
    "    sf_f.append(selected_features3)\n",
    "    \n",
    "    # Plot the scores\n",
    "    plt.figure()\n",
    "    plt.bar([i for i in range(len(f_selector3.scores_))], f_selector3.scores_)\n",
    "    plt.xlabel(f'feature index')\n",
    "    plt.ylabel(f'F-value')\n",
    "    plt.title(f'F-score Plots')\n",
    "    plt.savefig(f'Images/Feature Selection/F-scoreplots_3')\n",
    "    \n",
    "    return x1_f, t1_f, x2_f, t2_f, x3_f, t3_f, sf_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_f, testX1_f, X2_f, testX2_f, X3_f, testX3_f, selected_features_f = univariate_fregr(X1, X2, X3, \n",
    "                                                                    testX1, testX2, testX3, \n",
    "                                                                    y1, y2, y3, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55593ad9",
   "metadata": {},
   "source": [
    "### 3. Recursive Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_FS(model, x1, x2, x3, t1, t2, t3, Y1, Y2, Y3, feat1, feat3):\n",
    "    sf_rfe = []\n",
    "    rfe1 = RFE(estimator=model, step=1)\n",
    "    rfe1 = rfe1.fit(x1, Y1)\n",
    "    sf1 = pd.DataFrame({'Feature': list(feat1),\n",
    "                        'Ranking': rfe1.ranking_})\n",
    "    sf1 = sf1.sort_values(by='Ranking')\n",
    "    print(f'score for the transformed feature space: {rfe1.score(x1, Y1)}')\n",
    "    x1_rfe = rfe1.transform(x1)\n",
    "    t1_rfe = rfe1.transform(t1)\n",
    "    sf_rfe.append(list(sf1['Feature']))\n",
    "    \n",
    "    rfe2 = RFE(estimator=model, step=1)\n",
    "    rfe2 = rfe2.fit(x2, Y2)\n",
    "    sf2 = pd.DataFrame({'Feature': list(feat1),\n",
    "                        'Ranking': rfe2.ranking_})\n",
    "    sf2 = sf2.sort_values(by='Ranking')\n",
    "    print(f'score for the transformed feature space: {rfe2.score(x2, Y2)}')\n",
    "    x2_rfe = rfe2.transform(x2)\n",
    "    t2_rfe = rfe2.transform(t2)\n",
    "    sf_rfe.append(list(sf2['Feature']))\n",
    "    \n",
    "    rfe3 = RFE(estimator=model, step=1)\n",
    "    rfe3 = rfe3.fit(x3, Y3)\n",
    "    sf3 = pd.DataFrame({'Feature': list(feat3),\n",
    "                        'Ranking': rfe3.ranking_})\n",
    "    sf3 = sf3.sort_values(by='Ranking')\n",
    "    print(f'score for the transformed feature space: {rfe3.score(x3, Y3)}')\n",
    "    x3_rfe = rfe3.transform(x3)\n",
    "    t3_rfe = rfe3.transform(t3)\n",
    "    sf_rfe.append(list(sf3['Feature']))\n",
    "    \n",
    "    return x1_rfe, t1_rfe, x2_rfe, t2_rfe, x3_rfe, t3_rfe, sf_rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f775ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "linregr = LinearRegression()\n",
    "X1_rfe, testX1_rfe, X2_rfe, testX2_rfe, X3_rfe, testX3_rfe, selected_features_rfe = recursive_FS(linregr, X1, X2, X3, \n",
    "                                                                          testX1, testX2, testX3, \n",
    "                                                                          y1, y2, y3, features1, features3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ed6cf0",
   "metadata": {},
   "source": [
    "### 4. Recursive Feature Elimination Cross Validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388231f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_FS_CV(model, x1, x2, x3, t1, t2, t3, Y1, Y2, Y3):\n",
    "    sf_rfecv = []\n",
    "    rfecv = RFECV(estimator=model, step=1, min_features_to_select=5, cv=12, scoring='r2')\n",
    "    rfecv1 = rfecv.fit(x1, Y1)\n",
    "    print(f'Optimal number of features: {rfecv1.n_features_}')\n",
    "    print(f'Best features; {x1.columns[rfecv1.support_]}')\n",
    "    x1_rfecv = rfecv1.transform(x1)\n",
    "    t1_rfecv = rfecv1.transform(t1)\n",
    "    plt.figure()\n",
    "    plt.xlabel(f'Number of features selected')\n",
    "    plt.ylabel(f'Cross validation score of number of selected features')\n",
    "    plt.plot(range(1, len(rfecv1.grid_scores_) + 1), rfecv1.grid_scores_)\n",
    "    plt.title(f'Feature Ranking')\n",
    "    plt.savefig(f'Images/Feature Selection/RFECV_CVscore_mission1.png')\n",
    "    plt.show()\n",
    "    print(f'Score for RFECV1: {rfecv1.score(x1, Y1)}')\n",
    "    selected_features1 = x1.columns[rfecv1.support_]\n",
    "    sf_rfecv.append(selected_features1)\n",
    "    \n",
    "    rfecv2 = rfecv.fit(x2, Y2)\n",
    "    print(f'Optimal number of features: {rfecv2.n_features_}')\n",
    "    print(f'Best features; {x2.columns[rfecv2.support_]}')\n",
    "    x2_rfecv = rfecv2.transform(x2)\n",
    "    t2_rfecv = rfecv2.transform(t2)\n",
    "    plt.figure()\n",
    "    plt.xlabel(f'Number of features selected')\n",
    "    plt.ylabel(f'Cross validation score of number of selected features')\n",
    "    plt.plot(range(1, len(rfecv2.grid_scores_) + 1), rfecv2.grid_scores_)\n",
    "    plt.title(f'Feature Ranking')\n",
    "    plt.savefig(f'Images/Feature Selection/RFECV_CVscore_mission2.png')\n",
    "    plt.show()\n",
    "    print(f'Score for RFECV2: {rfecv2.score(x2, Y2)}')\n",
    "    selected_features2 = x2.columns[rfecv2.support_]\n",
    "    sf_rfecv.append(selected_features2)\n",
    "    \n",
    "    rfecv3 = rfecv.fit(x3, Y3)\n",
    "    print(f'Optimal number of features: {rfecv3.n_features_}')\n",
    "    print(f'Best features; {x3.columns[rfecv3.support_]}')\n",
    "    x3_rfecv = rfecv3.transform(x3)\n",
    "    t3_rfecv = rfecv3.transform(t3)\n",
    "    plt.figure()\n",
    "    plt.xlabel(f'Number of features selected')\n",
    "    plt.ylabel(f'Cross validation score of number of selected features')\n",
    "    plt.title(f'Feature Ranking')\n",
    "    plt.plot(range(1, len(rfecv3.grid_scores_) + 1), rfecv3.grid_scores_)\n",
    "    plt.savefig(f'Images/Feature Selection/RFECV_CVscore_mission3.png')\n",
    "    plt.show()\n",
    "    print(f'Score for RFECV3: {rfecv3.score(x3, Y3)}')\n",
    "    selected_features3 = x3.columns[rfecv3.support_]\n",
    "    sf_rfecv.append(selected_features3)\n",
    "    \n",
    "    return x1_rfecv, t1_rfecv, x2_rfecv, t2_rfecv, x3_rfecv, t3_rfecv, sf_rfecv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8963c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_rfecv, testX1_rfecv, X2_rfecv, testX2_rfecv, X3_rfecv, testX3_rfecv, selected_features_rfecv = recursive_FS_CV(linregr, X1, X2, X3, \n",
    "                                                                                         testX1, testX2, testX3, \n",
    "                                                                                         y1, y2, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ab153",
   "metadata": {},
   "source": [
    "### 5. Mutual information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3157b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info(x1, x2, x3, t1, t2, t3, Y1, Y2, Y3, k):\n",
    "    sf_mi = []\n",
    "    f_selector = SelectKBest(score_func=mutual_info_regression, k=k)\n",
    "    \n",
    "    # fit the data to the selector\n",
    "    f_selector1 = f_selector.fit(x1, Y1)\n",
    "    \n",
    "    # Transform\n",
    "    x1_mi = f_selector1.transform(x1)\n",
    "    \n",
    "    # Transform test input data\n",
    "    t1_mi = f_selector1.transform(t1)\n",
    "    \n",
    "    selected_features1 = [features1[i] for i in f_selector1.scores_.argsort()[::-1][:k]]\n",
    "    sf_mi.append(selected_features1)\n",
    "    \n",
    "    plt.figure()\n",
    "    # Plot scores\n",
    "    plt.bar([i for i in range(len(f_selector1.scores_))], f_selector1.scores_)\n",
    "    plt.xlabel(f'feature index')\n",
    "    plt.ylabel(f'Estimated Mutual Index value')\n",
    "    plt.title(f'Mutual Information')\n",
    "    plt.savefig(f'Images/Feature Selection/mutual_info_1.png')\n",
    "    \n",
    "    # fit the data to the selector\n",
    "    f_selector2 = f_selector.fit(x2, Y2)\n",
    "    \n",
    "    # Transform\n",
    "    x2_mi = f_selector2.transform(x2)\n",
    "    \n",
    "    # Transform test input data\n",
    "    t2_mi = f_selector2.transform(t2)\n",
    "    \n",
    "    selected_features2 = [features2[i] for i in f_selector2.scores_.argsort()[::-1][:k]]\n",
    "    sf_mi.append(selected_features2)\n",
    "    \n",
    "    plt.figure()\n",
    "    # Plot scores\n",
    "    plt.bar([i for i in range(len(f_selector2.scores_))], f_selector2.scores_)\n",
    "    plt.xlabel(f'feature index')\n",
    "    plt.ylabel(f'Estimated Mutual Index value')\n",
    "    plt.title(f'Mutual Information')\n",
    "    plt.savefig(f'Images/Feature Selection/mutual_info_2.png')\n",
    "    \n",
    "    # fit the data to the selector\n",
    "    f_selector3 = f_selector.fit(x3, Y3)\n",
    "    \n",
    "    # Transform\n",
    "    x3_mi = f_selector3.transform(x3)\n",
    "    \n",
    "    # Transform test input data\n",
    "    t3_mi = f_selector3.transform(t3)\n",
    "    \n",
    "    selected_features3 = [features3[i] for i in f_selector3.scores_.argsort()[::-1][:k]]\n",
    "    sf_mi.append(selected_features3)\n",
    "    \n",
    "    plt.figure()\n",
    "    # Plot scores\n",
    "    plt.bar([i for i in range(len(f_selector3.scores_))], f_selector3.scores_)\n",
    "    plt.xlabel(f'feature index')\n",
    "    plt.ylabel(f'Estimated Mutual Index value')\n",
    "    plt.title(f'Mutual Information')\n",
    "    plt.savefig(f'Images/Feature Selection/mutual_info_3.png')\n",
    "    \n",
    "    sf = set(selected_features3).intersection(list(set(selected_features1).intersection(selected_features2)))\n",
    "    \n",
    "    return x1_mi, t1_mi, x2_mi, t2_mi, x3_mi, t3_mi, sf_mi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aff950",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_mi, testX1_mi, X2_mi, testX2_mi, X3_mi, testX3_mi, selected_features_mi = mutual_info(X1, X2, X3, \n",
    "                                                                   testX1, testX2, testX3, \n",
    "                                                                   y1, y2, y3, 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174eab56",
   "metadata": {},
   "source": [
    "## MODEL SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bea443",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the common features selected by all the models\n",
    "def get_common_features(sf1, sf2, sf3, sf4, sf5):\n",
    "    comm_feat = []\n",
    "    for i in range(len(sf1)):\n",
    "        intersection_list1 = set(sf1[i])\n",
    "        intersection_list2 = set(intersection_list1.intersection(set(sf2[i])))                                     \n",
    "        intersection_list3 = set(intersection_list2.intersection(set(sf3[i])))\n",
    "        intersection_list4 = set(intersection_list3.intersection(set(sf4[i]))) \n",
    "        intersection_list5 = set(intersection_list4.intersection(set(sf5[i])))  \n",
    "        comm_feat.append(intersection_list5)\n",
    "    return comm_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = get_common_features(sf_p, selected_features_f, selected_features_rfe, selected_features_rfecv, selected_features_mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a532c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X1 = X1[sf[0]]\n",
    "new_X2 = X2[sf[1]]\n",
    "new_X3 = X3[sf[2]]\n",
    "\n",
    "new_testX1 = testX1[sf[0]]\n",
    "new_testX2 = testX2[sf[1]]\n",
    "new_testX3 = testX3[sf[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b251106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store R2_scores for each model and different Features Selections\n",
    "SVR_R2, lr_R2, KNN_R2, NN_R2 = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09100a58",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(x, t, Y):\n",
    "    y_mean = np.mean(Y)\n",
    "    y_std = np.std(Y)\n",
    "    for i in range(x.shape[1]):\n",
    "        x_mean = np.mean(x[:,i])\n",
    "        x_std = np.std(x[:,i])\n",
    "        for j in range(x.shape[0]):\n",
    "            x[j,i] = (x[j,i]-x_mean)/x_std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(model, x, Y, t, tY):\n",
    "#     regr = make_pipeline(StandardScaler(), model)\n",
    "    regr = make_pipeline(model)\n",
    "    cv_regr = cross_validate(regr, np.array(x), Y, cv=5, return_train_score=True, return_estimator=True)\n",
    "    val_scores = cv_regr['test_score']\n",
    "    train_scores = cv_regr['train_score']\n",
    "    optimal_estimator = cv_regr['estimator'][np.argmax(val_scores)]\n",
    "    \n",
    "    return np.mean(val_scores), np.mean(train_scores), optimal_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d99c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores for different parameters\n",
    "def plot_model_param(x, val_scores, tr_scores, m):\n",
    "    plt.figure()\n",
    "    plt.plot(x, val_scores, 'b', label='Val score')\n",
    "    plt.plot(x, tr_scores, 'r', label='Train score')\n",
    "    plt.title(f'r2 score plots - Mission_{m}')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(estimator, x, y, t, tY):\n",
    "    score_out = []\n",
    "    test_regr = estimator\n",
    "    test_regr.fit(x, y)\n",
    "    y_pred = test_regr.predict(t)\n",
    "    error = error_estimation(np.array(tY))\n",
    "    print(f'RMSE: {error.RMSE(y_pred)}')\n",
    "    score_out.append(error.RMSE(y_pred))\n",
    "    print(f'MAE: {error.MAE(y_pred)}')\n",
    "    score_out.append(error.MAE(y_pred))\n",
    "    print(f'R2: {test_regr.score(t, tY)}') \n",
    "    score_out.append(test_regr.score(t, tY))\n",
    "        \n",
    "    return y_pred, score_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf56e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_dist(y_true, y_pred, m):   \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9,5))\n",
    "    ax1.hist(y_true, edgecolor='white')\n",
    "    ax1.set_title(f'Actual G1 Distribution')\n",
    "    ax2.hist(y_pred, edgecolor='white')\n",
    "    ax2.set_title(f'Predicted G1 Distribution')\n",
    "    fig.suptitle(f'MISSION{m}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1428e72",
   "metadata": {},
   "source": [
    "### 1. KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1170ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_SCORE1, knn_SCORE2, knn_SCORE3 = [], [], []\n",
    "knn_tr_SCORE1, knn_tr_SCORE2, knn_tr_SCORE3 = [], [], []\n",
    "ESTIMATOR1, ESTIMATOR2, ESTIMATOR3 = [], [], []\n",
    "\n",
    "samples = np.arange(1, 30, 1)\n",
    "for k in samples:\n",
    "    knn_score1, train_score1, estimator1 = model_selection(KNeighborsRegressor(n_neighbors=k), X1_rfe, y1, testX1_rfe, testy1)\n",
    "    knn_SCORE1.append(svr_score1)\n",
    "    ESTIMATOR1.append(estimator1)\n",
    "    knn_tr_SCORE1.append(train_score1)\n",
    "    \n",
    "    knn_score2, train_score2, estimator2 = model_selection(KNeighborsRegressor(n_neighbors=k), X2_rfe, y2, testX2_rfe, testy2)\n",
    "    knn_SCORE2.append(svr_score2)\n",
    "    ESTIMATOR2.append(estimator2)\n",
    "    knn_tr_SCORE2.append(train_score2)\n",
    "    \n",
    "    knn_score3, train_score3, estimator3 = model_selection(KNeighborsRegressor(n_neighbors=k), X3_rfe, y3, testX3_rfe, testy3)\n",
    "    knn_SCORE3.append(svr_score3)\n",
    "    ESTIMATOR3.append(estimator3)\n",
    "    knn_tr_SCORE3.append(train_score3)\n",
    "\n",
    "optim_est1 = ESTIMATOR1[np.argmax(knn_SCORE1)]\n",
    "plot_model_param(samples, knn_SCORE1, knn_tr_SCORE1, m=1)\n",
    "optimal_param1 = samples[np.argmax(knn_SCORE1)]\n",
    "print(f'optimal_epsilon1: {optimal_param1}')\n",
    "print(f'optimal r2_score for Mission1: {np.max(knn_SCORE1)}')\n",
    "plt.savefig(f'Images/Results/knn_mission1_scores.png')\n",
    "\n",
    "optim_est2 = ESTIMATOR2[np.argmax(knn_SCORE2)]\n",
    "plot_model_param(samples, knn_SCORE2, knn_tr_SCORE2, m=2)\n",
    "optimal_param2 = samples[np.argmax(knn_SCORE2)]\n",
    "print(f'optimal_epsilon2: {optimal_param2}')\n",
    "print(f'optimal r2_score for Mission2: {np.max(knn_SCORE2)}')\n",
    "plt.savefig(f'Images/Results/knn_mission2_scores.png')\n",
    "\n",
    "optim_est3 = ESTIMATOR3[np.argmax(knn_SCORE3)]\n",
    "plot_model_param(samples, knn_SCORE3, knn_tr_SCORE1, m=3)\n",
    "optimal_param3 = samples[np.argmax(knn_SCORE3)]\n",
    "print(f'optimal_epsilon1: {optimal_param3}')\n",
    "print(f'optimal r2_score for Mission3: {np.max(knn_SCORE3)}')\n",
    "plt.savefig(f'Images/Results/knn_mission3_scores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_knn = []\n",
    "print(f'Mission1')\n",
    "y_pred1, score_out1 = test_model(optim_est1, X1_rfe, y1, testX1_rfe, np.array(testy1))\n",
    "score_knn.append(score_out1)\n",
    "print(f'Mission2')\n",
    "y_pred2, score_out2 = test_model(optim_est2, X2_rfe, y2, testX2_rfe, np.array(testy2))\n",
    "score_knn.append(score_out2)\n",
    "print(f'Mission3')\n",
    "y_pred3, score_out3 = test_model(optim_est3, X3_rfe, y3, testX3_rfe, np.array(testy3))\n",
    "score_knn.append(score_out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab82e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_dist(y1, y_pred1, 1)\n",
    "plt.savefig(f'Images/Results/knn_M1_dist.png')\n",
    "plot_pred_dist(y2, y_pred2, 2)\n",
    "plt.savefig(f'Images/Results/knn_M2_dist.png')\n",
    "plot_pred_dist(y3, y_pred3, 3)\n",
    "plt.savefig(f'Images/Results/knn_M3_dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc3c6c",
   "metadata": {},
   "source": [
    "### 2. Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786cf8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svr_SCORE1, svr_SCORE2, svr_SCORE3 = [], [], []\n",
    "svr_tr_SCORE1, svr_tr_SCORE2, svr_tr_SCORE3 = [], [], []\n",
    "ESTIMATOR1, ESTIMATOR2, ESTIMATOR3 = [], [], []\n",
    "\n",
    "samples = np.arange(0.01, 50, 0.1)\n",
    "for c in samples:\n",
    "    svr_score1, train_score1, estimator1 = model_selection(SVR(C=c, epsilon=0.1), X1_rfe, y1, testX1_rfe, testy1)\n",
    "    svr_SCORE1.append(svr_score1)\n",
    "    ESTIMATOR1.append(estimator1)\n",
    "    svr_tr_SCORE1.append(train_score1)\n",
    "    svr_score2, train_score2, estimator2 = model_selection(SVR(C=c, epsilon=0.1), X2_rfe, y2, testX2_rfe, testy2)\n",
    "    svr_SCORE2.append(svr_score2)\n",
    "    ESTIMATOR2.append(estimator2)\n",
    "    svr_tr_SCORE2.append(train_score2)\n",
    "    svr_score3, train_score3, estimator3 = model_selection(SVR(C=c, epsilon=0.1), X3_rfe, y3, testX3_rfe, testy3)\n",
    "    svr_SCORE3.append(svr_score3)\n",
    "    ESTIMATOR3.append(estimator3)\n",
    "    svr_tr_SCORE3.append(train_score3)\n",
    "\n",
    "optim_est1 = ESTIMATOR1[np.argmax(svr_SCORE1)]\n",
    "plot_model_param(samples, svr_SCORE1, svr_tr_SCORE1, m=1)\n",
    "optimal_param1 = samples[np.argmax(svr_SCORE1)]\n",
    "print(f'optimal_epsilon1: {optimal_param1}')\n",
    "print(f'optimal r2_score for Mission1: {np.max(svr_SCORE1)}')\n",
    "plt.savefig(f'Images/Results/svr_mission1_scores.png')\n",
    "\n",
    "optim_est2 = ESTIMATOR2[np.argmax(svr_SCORE2)]\n",
    "plot_model_param(samples, svr_SCORE2, svr_tr_SCORE2, m=2)\n",
    "optimal_param2 = samples[np.argmax(svr_SCORE2)]\n",
    "print(f'optimal_epsilon2: {optimal_param2}')\n",
    "print(f'optimal r2_score for Mission2: {np.max(svr_SCORE2)}')\n",
    "plt.savefig(f'Images/Results/svr_mission2_scores.png')\n",
    "\n",
    "optim_est3 = ESTIMATOR3[np.argmax(svr_SCORE3)]\n",
    "plot_model_param(samples, svr_SCORE3, svr_tr_SCORE1, m=3)\n",
    "optimal_param3 = samples[np.argmax(svr_SCORE3)]\n",
    "print(f'optimal_epsilon1: {optimal_param3}')\n",
    "print(f'optimal r2_score for Mission3: {np.max(svr_SCORE3)}')\n",
    "plt.savefig(f'Images/Results/svr_mission3_scores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_svr = []\n",
    "print(f'Mission1')\n",
    "y_pred1, score_out1 = test_model(optim_est1, X1_rfe, y1, testX1_rfe, np.array(testy1))\n",
    "score_svr.append(score_out1)\n",
    "print(f'Mission2')\n",
    "y_pred2, score_out2 = test_model(optim_est2, X2_rfe, y2, testX2_rfe, np.array(testy2))\n",
    "score_svr.append(score_out2)\n",
    "print(f'Mission3')\n",
    "y_pred3, score_out3 = test_model(optim_est3, X3_rfe, y3, testX3_rfe, np.array(testy3))\n",
    "score_svr.append(score_out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_dist(y1, y_pred1, 1)\n",
    "plt.savefig(f'Images/Results/svr_M1_dist.png')\n",
    "plot_pred_dist(y2, y_pred2, 2)\n",
    "plt.savefig(f'Images/Results/svr_M2_dist.png')\n",
    "plot_pred_dist(y3, y_pred3, 3)\n",
    "plt.savefig(f'Images/Results/svr_M3_dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad536a",
   "metadata": {},
   "source": [
    "### 3. Ridge Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f064e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_SCORE1, lr_SCORE2, lr_SCORE3 = [], [], []\n",
    "lr_tr_SCORE1, lr_tr_SCORE2, lr_tr_SCORE3 = [], [], []\n",
    "ESTIMATOR1, ESTIMATOR2, ESTIMATOR3 = [], [], []\n",
    "\n",
    "samples = np.arange(0.01, 10, 0.1)\n",
    "for c in samples:\n",
    "    lr_score1, train_score1, estimator1 = model_selection(Ridge(c), X1_rfe, y1, testX1_rfe, testy1)\n",
    "    lr_SCORE1.append(lr_score1)\n",
    "    ESTIMATOR1.append(estimator1)\n",
    "    lr_tr_SCORE1.append(train_score1)\n",
    "    lr_score2, train_score2, estimator2 = model_selection(Ridge(c), X2_rfe, y2, testX2_rfe, testy2)\n",
    "    lr_SCORE2.append(lr_score2)\n",
    "    ESTIMATOR2.append(estimator2)\n",
    "    lr_tr_SCORE2.append(train_score2)\n",
    "    lr_score3, train_score3, estimator3 = model_selection(Ridge(c), X3_rfe, y3, testX3_rfe, testy3)\n",
    "    lr_SCORE3.append(lr_score3)\n",
    "    ESTIMATOR3.append(estimator3)\n",
    "    lr_tr_SCORE3.append(train_score3)\n",
    "\n",
    "optim_est1 = ESTIMATOR1[np.argmax(lr_SCORE1)]\n",
    "plot_model_param(samples, lr_SCORE1, lr_tr_SCORE1, m=1)\n",
    "optimal_param1 = samples[np.argmax(lr_SCORE1)]\n",
    "print(f'optimal_epsilon1: {optimal_param1}')\n",
    "print(f'optimal r2_score for Mission1: {np.max(lr_SCORE1)}')\n",
    "plt.savefig(f'Images/Results/rlr_mission1_scores.png')\n",
    "\n",
    "optim_est2 = ESTIMATOR2[np.argmax(lr_SCORE2)]\n",
    "plot_model_param(samples, lr_SCORE2, lr_tr_SCORE2, m=2)\n",
    "optimal_param2 = samples[np.argmax(lr_SCORE2)]\n",
    "print(f'optimal_epsilon2: {optimal_param2}')\n",
    "print(f'optimal r2_score for Mission2: {np.max(lr_SCORE2)}')\n",
    "plt.savefig(f'Images/Results/rlr_mission2_scores.png')\n",
    "\n",
    "optim_est3 = ESTIMATOR3[np.argmax(lr_SCORE3)]\n",
    "plot_model_param(samples, lr_SCORE3, lr_tr_SCORE1, m=3)\n",
    "optimal_param3 = samples[np.argmax(lr_SCORE3)]\n",
    "print(f'optimal_epsilon1: {optimal_param3}')\n",
    "print(f'optimal r2_score for Mission3: {np.max(lr_SCORE3)}')\n",
    "plt.savefig(f'Images/Results/knn_mission3_scores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_rlr = []\n",
    "print(f'Mission1')\n",
    "y_pred1, score_out1 = test_model(optim_est1, X1_rfe, y1, testX1_rfe, np.array(testy1))\n",
    "score_rlr.append(score_out1)\n",
    "print(f'Mission2')\n",
    "y_pred2, score_out2 = test_model(optim_est2, X2_rfe, y2, testX2_rfe, np.array(testy2))\n",
    "score_rlr.append(score_out2)\n",
    "print(f'Mission3')\n",
    "y_pred3, score_out3 = test_model(optim_est3, X3_rfe, y3, testX3_rfe, np.array(testy3))\n",
    "score_rlr.append(score_out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77346a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_dist(y1, y_pred1, 1)\n",
    "plt.savefig(f'Images/Results/rlr_M1_dist.png')\n",
    "plot_pred_dist(y2, y_pred2, 2)\n",
    "plt.savefig(f'Images/Results/rlr_M2_dist.png')\n",
    "plot_pred_dist(y3, y_pred3, 3)\n",
    "plt.savefig(f'Images/Results/rlr_M3_dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392d5a2",
   "metadata": {},
   "source": [
    "### 4. ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a67d258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_SCORE1, nn_SCORE2, nn_SCORE3 = [], [], []\n",
    "nn_tr_SCORE1, nn_tr_SCORE2, nn_tr_SCORE3 = [], [], []\n",
    "ESTIMATOR1, ESTIMATOR2, ESTIMATOR3 = [], [], []\n",
    "\n",
    "samples = np.arange(100, 120, 1)\n",
    "for c in samples:\n",
    "    nn_score1, train_score1, estimator1 = model_selection(MLPRegressor(hidden_layer_sizes=(c,)), X1_rfe, y1, testX1_rfe, testy1)\n",
    "    nn_SCORE1.append(nn_score1)\n",
    "    ESTIMATOR1.append(estimator1)\n",
    "    nn_tr_SCORE1.append(train_score1)\n",
    "    nn_score2, train_score2, estimator2 = model_selection(MLPRegressor(hidden_layer_sizes=(c,)), X2_rfe, y2, testX2_rfe, testy2)\n",
    "    nn_SCORE2.append(nn_score2)\n",
    "    ESTIMATOR2.append(estimator2)\n",
    "    nn_tr_SCORE2.append(train_score2)\n",
    "    nn_score3, train_score3, estimator3 = model_selection(MLPRegressor(hidden_layer_sizes=(c,)), X3_rfe, y3, testX3_rfe, testy3)\n",
    "    nn_SCORE3.append(nn_score3)\n",
    "    ESTIMATOR3.append(estimator3)\n",
    "    nn_tr_SCORE3.append(train_score3)\n",
    "\n",
    "optim_est1 = ESTIMATOR1[np.argmax(nn_SCORE1)]\n",
    "plot_model_param(samples, nn_SCORE1, nn_tr_SCORE1, m=1)\n",
    "optimal_param1 = samples[np.argmax(nn_SCORE1)]\n",
    "print(f'optimal_epsilon1: {optimal_param1}')\n",
    "print(f'optimal r2_score for Mission1: {np.max(nn_SCORE1)}')\n",
    "plt.savefig(f'Images/Results/nn_mission1_scores.png')\n",
    "\n",
    "optim_est2 = ESTIMATOR2[np.argmax(nn_SCORE2)]\n",
    "plot_model_param(samples, nn_SCORE2, nn_tr_SCORE2, m=2)\n",
    "optimal_param2 = samples[np.argmax(nn_SCORE2)]\n",
    "print(f'optimal_epsilon2: {optimal_param2}')\n",
    "print(f'optimal r2_score for Mission2: {np.max(nn_SCORE2)}')\n",
    "plt.savefig(f'Images/Results/nn_mission2_scores.png')\n",
    "\n",
    "optim_est3 = ESTIMATOR3[np.argmax(nn_SCORE3)]\n",
    "# plot_model_param(samples, lr_SCORE3, nn_tr_SCORE1, m=3)\n",
    "optimal_param3 = samples[np.argmax(nn_SCORE3)]\n",
    "print(f'optimal_epsilon1: {optimal_param3}')\n",
    "print(f'optimal r2_score for Mission3: {np.max(nn_SCORE3)}')\n",
    "# plt.savefig(f'Images/Results/nn_mission3_scores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e72567",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_nn = []\n",
    "print(f'Mission1')\n",
    "y_pred1, score_out1 = test_model(optim_est1, X1_rfe, y1, testX1_rfe, np.array(testy1))\n",
    "score_nn.append(score_out1)\n",
    "print(f'Mission2')\n",
    "y_pred2, score_out2 = test_model(optim_est2, X2_rfe, y2, testX2_rfe, np.array(testy2))\n",
    "score_nn.append(score_out2)\n",
    "print(f'Mission3')\n",
    "y_pred3, score_out3 = test_model(optim_est3, X3_rfe, y3, testX3_rfe, np.array(testy3))\n",
    "score_nn.append(score_out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd4892",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_dist(y1, y_pred1, 1)\n",
    "plt.savefig(f'Images/Results/nn_M1_dist.png')\n",
    "plot_pred_dist(y2, y_pred2, 2)\n",
    "plt.savefig(f'Images/Results/nn_M2_dist.png')\n",
    "plot_pred_dist(y3, y_pred3, 3)\n",
    "plt.savefig(f'Images/Results/nn_M3_dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898fbc53",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comp(score1, score2, score3, score4, m):\n",
    "    plt.figure()\n",
    "    plt.rcdefaults()\n",
    "    w = 0.2\n",
    "    models = [\"KNN\", \"SVR\", \"Ridge\", \"NN\"]\n",
    "    ypos1 = np.arange(len(score1))\n",
    "    ypos2 = [i+w for i in ypos1]\n",
    "    ypos3 = [i+w for i in ypos2]\n",
    "    ypos4 = [i+w for i in ypos3]\n",
    "    plt.bar(ypos1, score1, width=w, label=\"KNN\")\n",
    "    plt.bar(ypos2, score2, width=w, label=\"SVR\")\n",
    "    plt.bar(ypos3, score3, width=w, label=\"Ridge\")\n",
    "    plt.bar(ypos4, score4, width=w, label=\"NN\")\n",
    "    plt.legend()\n",
    "    plt.title(m+' for all the Missions')\n",
    "    plt.savefig('Images/Results/Final_score_plots_' + m + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a348f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comp(score_knn[:][0], score_svr[:][0], score_rlr[:][0], score_nn[:][0], \"RMSE\")\n",
    "plot_model_comp(score_knn[:][1], score_svr[:][1], score_knn[:][1], score_nn[:][1], \"MAE\")\n",
    "plot_model_comp(score_knn[:][2], score_svr[:][2], score_knn[:][2], score_nn[:][2], \"R2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
